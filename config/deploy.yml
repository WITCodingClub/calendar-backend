# Name of your application. Used to uniquely configure containers.
service: wit_calendar_backend

# Name of the container image (without registry prefix - server is in registry config)
image: witcodingclub/calendar-backend

# Proxy configuration - kamal-proxy listens on port 3002, Caddy proxies to it
proxy:
  host: 0.0.0.0
  app_port: 3000
  publish:
    - "3002:80"  # kamal-proxy listens on host port 3002
  ssl: false  # Caddy handles SSL, not kamal-proxy

# Deploy to these servers.
servers:
  web:
    hosts:
      - alastor
  # job:
  #   hosts:
  #     - alastor
  #   cmd: bin/jobs

# Credentials for GitHub Container Registry
registry:
  server: ghcr.io
  username: jaspermayone

  # GitHub Personal Access Token with packages:write permission
  password:
    - KAMAL_REGISTRY_PASSWORD

# Inject ENV variables into containers (secrets come from .kamal/secrets).
env:
  secret:
    - RAILS_MASTER_KEY
    - DB_PASSWORD
  clear:
    # Run the Solid Queue Supervisor inside the web server's Puma process to do jobs.
    # When you start using multiple servers, you should split out job processing to a dedicated machine.
    SOLID_QUEUE_IN_PUMA: true

    # Database connection via Kamal Docker network
    # Kamal accessories are accessible by their accessory name on the kamal network
    DB_HOST: wit_calendar_backend-db
    DB_PORT: 5432
    DB_NAME: wit_calendar_backend_production
    DB_USERNAME: wit_calendar_backend
    DB_NAME_QUEUE: wit_calendar_backend_production_queue

    # Redis connection via Kamal Docker network
    REDIS_URL: redis://wit_calendar_backend-redis:6379/0

    # Set number of processes dedicated to Solid Queue (default: 1)
    # JOB_CONCURRENCY: 3

    # Set number of cores available to the application on each server (default: 1).
    # WEB_CONCURRENCY: 2

    # Log everything from Rails
    # RAILS_LOG_LEVEL: debug

# Aliases are triggered with "bin/kamal <alias>". You can overwrite arguments on invocation:
# "bin/kamal logs -r job" will tail logs from the first server in the job section.
aliases:
  console: app exec --interactive --reuse "bin/rails console"
  shell: app exec --interactive --reuse "bash"
  logs: app logs -f
  dbc: app exec --interactive --reuse "bin/rails dbconsole"


# Use a persistent storage volume for sqlite database files and local Active Storage files.
# Recommended to change this to a mounted volume path that is backed up off server.
volumes:
  - "wit_calendar_backend_storage:/rails/storage"


# Bridge fingerprinted assets, like JS and CSS, between versions to avoid
# hitting 404 on in-flight requests. Combines all files from new and old
# version inside the asset_path.
asset_path: /rails/public/assets

# Configure the image builder.
builder:
  # Build for ARM64 since alastor is ARM-based
  arch: arm64

  # Pass git SHA to Docker build for version display
  args:
    GIT_SHA: <%= `git rev-parse --short HEAD`.strip %>

  # # Build image via remote server (useful for faster amd64 builds on arm64 computers)
  # remote: ssh://docker@docker-builder-server

# Use jsp user instead of root
ssh:
  user: jsp

# Use accessory services (secrets come from .kamal/secrets).
accessories:
  db:
    image: pgvector/pgvector:pg17
    host: alastor
    network: kamal
    port: "127.0.0.1:5432:5432"
    env:
      clear:
        POSTGRES_DB: wit_calendar_backend_production
        POSTGRES_USER: wit_calendar_backend
      secret:
        - POSTGRES_PASSWORD
    directories:
      - data:/var/lib/postgresql/data
  redis:
    image: redis:7-alpine
    host: alastor
    network: kamal
    port: "127.0.0.1:6379:6379"
    directories:
      - data:/data
